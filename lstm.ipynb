{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1146, -0.2298, -0.0572,  0.0379, -0.1007],\n",
      "         [ 0.0352, -0.2922,  0.0129,  0.0849, -0.0091],\n",
      "         [ 0.0983, -0.2999,  0.0324,  0.1157, -0.1778]],\n",
      "\n",
      "        [[-0.2604,  0.1363, -0.0916, -0.0806, -0.2363],\n",
      "         [-0.0607, -0.0717, -0.0270, -0.0547, -0.0796],\n",
      "         [-0.1115,  0.0628, -0.0180,  0.0091,  0.0401]]],\n",
      "       grad_fn=<TransposeBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nweight_ih_l0 torch.Size([20, 4])    对应各个w_i拼接 与input相乘 h_size*4, i_size\\nweight_hh_l0 torch.Size([20, 5])    对应各个w_h拼接 与上一时刻隐藏状态相乘 h_size*4, h_size\\nbias_ih_l0 torch.Size([20])         对应b_i\\nbias_hh_l0 torch.Size([20])         对应b_h\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lstm & lstmp 源码\n",
    "# 常量：batchsize, time, inputsizt, hiddensize\n",
    "bs, T, i_size, h_size = 2, 3, 4, 5\n",
    "# proj_size = \n",
    "input = torch.randn(bs, T, i_size)  # 正态分布 输入序列\n",
    "c0 = torch.randn(bs, h_size)        # 只考虑一层，初始值，不参与训练\n",
    "h0 = torch.randn(bs, h_size)        # 初始值\n",
    "\n",
    "# 调用官方API\n",
    "lstm_layer = nn.LSTM(i_size, h_size, batch_first=True)\n",
    "output, (h_final, c_final) = lstm_layer(input, (h0.unsqueeze(0), c0.unsqueeze(0)))       # 输入里的元组维度是三维\n",
    "print(output)\n",
    "# print(output)\n",
    "\n",
    "# for k, v in lstm_layer.named_parameters():\n",
    "#     print(k, v.shape)\n",
    "# 打印后可以看到有4个参数，是拼接的\n",
    "'''\n",
    "weight_ih_l0 torch.Size([20, 4])    对应各个w_i拼接 与input相乘 h_size*4, i_size\n",
    "weight_hh_l0 torch.Size([20, 5])    对应各个w_h拼接 与上一时刻隐藏状态相乘 h_size*4, h_size\n",
    "bias_ih_l0 torch.Size([20])         对应b_i\n",
    "bias_hh_l0 torch.Size([20])         对应b_h\n",
    "'''\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-20220903161843731](123/image-20220903161843731.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手撸版\n",
    "def lstm_forward(input, initial_state, w_ih, w_hh, b_ih, b_hh):\n",
    "    h0, c0 = initial_state  # 初始状态\n",
    "    bs, T, i_size = input.shape\n",
    "    h_szie = w_ih.shape[0] // 4\n",
    "    \n",
    "    prev_h = h0\n",
    "    prev_c = c0\n",
    "    # 扩维\n",
    "    batch_w_ih = w_ih.unsqueeze(0).tile(bs, 1, 1)  # [bs, 4*h_size, i_size]\n",
    "    batch_w_hh = w_hh.unsqueeze(0).tile(bs, 1, 1)  # [bs, 4*h_size, h_size]\n",
    "\n",
    "    output_size = h_size\n",
    "    output = torch.zeros(bs, T, output_size)    # 输出矩阵\n",
    "\n",
    "    for t in range(T):\n",
    "        x = input[:, t, :] # 当前时刻输入向量 [bs, i_size] 最后要扩一维\n",
    "        w_times_x = torch.bmm(batch_w_ih, x.unsqueeze(-1))   # [bs, 4*h_size, 1]\n",
    "        w_times_x = w_times_x.squeeze(-1)  # [bs, 4*h_size]\n",
    "\n",
    "        w_times_h_prev = torch.bmm(batch_w_hh, prev_h.unsqueeze(-1))\n",
    "        w_times_h_prev = w_times_h_prev.squeeze(-1)\n",
    "        \n",
    "        # 分别计算输入门i、遗忘门f、cell门g、输出门o\n",
    "        i_t = torch.sigmoid(w_times_x[:, :h_size] + w_times_h_prev[:, :h_size] + b_ih[:h_size] + b_hh[:h_size])\n",
    "        f_t = torch.sigmoid(w_times_x[:, h_size:2*h_size] + w_times_h_prev[:, h_size:2*h_size] +\\\n",
    "             b_ih[h_size:2*h_size] + b_hh[h_size:2*h_size])\n",
    "        g_t = torch.tanh(w_times_x[:, 2*h_size:3*h_size] + w_times_h_prev[:, 2*h_size:3*h_size] +\\\n",
    "             b_ih[2*h_size:3*h_size] + b_hh[2*h_size:3*h_size])\n",
    "        o_t = torch.sigmoid(w_times_x[:, 3*h_size:4*h_size] + w_times_h_prev[:, 3*h_size:4*h_size] +\\\n",
    "             b_ih[3*h_size:4*h_size] + b_hh[3*h_size:4*h_size])\n",
    "\n",
    "        # 更新\n",
    "        prev_c = f_t * prev_c + i_t * g_t\n",
    "        prev_h = o_t * torch.tanh(prev_c)\n",
    "\n",
    "        output[:, t, :] = prev_h\n",
    "\n",
    "    return output, (prev_h,prev_c)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1146, -0.2298, -0.0572,  0.0379, -0.1007],\n",
      "         [ 0.0352, -0.2922,  0.0129,  0.0849, -0.0091],\n",
      "         [ 0.0983, -0.2999,  0.0324,  0.1157, -0.1778]],\n",
      "\n",
      "        [[-0.2604,  0.1363, -0.0916, -0.0806, -0.2363],\n",
      "         [-0.0607, -0.0717, -0.0270, -0.0547, -0.0796],\n",
      "         [-0.1115,  0.0628, -0.0180,  0.0091,  0.0401]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[[ 0.1146, -0.2298, -0.0572,  0.0379, -0.1007],\n",
      "         [ 0.0352, -0.2922,  0.0129,  0.0849, -0.0091],\n",
      "         [ 0.0983, -0.2999,  0.0324,  0.1157, -0.1778]],\n",
      "\n",
      "        [[-0.2604,  0.1363, -0.0916, -0.0806, -0.2363],\n",
      "         [-0.0607, -0.0717, -0.0270, -0.0547, -0.0796],\n",
      "         [-0.1115,  0.0628, -0.0180,  0.0091,  0.0401]]], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "# 对比验证\n",
    "print(output)\n",
    "# 将参数直接放入\n",
    "output1, (h_final1, c_final1) = lstm_forward(input, (h0, c0), lstm_layer.weight_ih_l0, lstm_layer.weight_hh_l0, lstm_layer.bias_ih_l0, lstm_layer.bias_hh_l0)\n",
    "print(output1)\n",
    "# 相同！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Net (nn.Module):\n",
    "    def __init__(self):             # 初始化\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input):       # 只需要正向传播，反向传播不用自己写\n",
    "        output = input + 1\n",
    "        return output               # 就是整个网络的返回值\n",
    "\n",
    "net = Net()\n",
    "x = torch.tensor(1.0)\n",
    "y = net(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('xsyML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0be4a6b9786a50d1bd1dd266be3b5ed561508e6b0a0236030e6ccf1d0c42e416"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
